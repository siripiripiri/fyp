{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy yake scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8fNeMa4b3vs",
        "outputId": "80e86a8a-060b-4949-e829-7f5903e3e2bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Collecting yake\n",
            "  Downloading yake-0.4.8-py2.py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.13.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from yake) (0.9.0)\n",
            "Requirement already satisfied: click>=6.0 in /usr/local/lib/python3.10/dist-packages (from yake) (8.1.7)\n",
            "Collecting segtok (from yake)\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from yake) (3.4.2)\n",
            "Requirement already satisfied: jellyfish in /usr/local/lib/python3.10/dist-packages (from yake) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from segtok->yake) (2024.9.11)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading yake-0.4.8-py2.py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: segtok, yake\n",
            "Successfully installed segtok-1.5.11 yake-0.4.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJ71w0DycQEA",
        "outputId": "ac602997-1c81-4cdd-f756-5df156e048ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.13.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxeFXhpmbV2x",
        "outputId": "318e7b44-7627-4276-d3e3-e4d0e6b7edb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced prompt for LLM:\n",
            "Context:\n",
            "Important concepts: Industrial Revolution began, began in Britain, fundamentally changing economic, major turning point, century\n",
            "DATE: the late 18th century\n",
            "GPE: Britain\n",
            "\n",
            "Original text:\n",
            "\n",
            "    The Industrial Revolution began in Britain in the late 18th century. \n",
            "    This period marked a major turning point in human history, fundamentally \n",
            "    changing economic and social organization. The transition included going \n",
            "    from manual production methods to machine manufacturing, new chemical \n",
            "    manufacturing and iron production processes, improved efficiency of water \n",
            "    power, the increasing use of steam power, and the development of machine tools.\n",
            "    \n",
            "\n",
            "Please generate questions focusing on the relationships between these key concepts and entities.\n",
            "Consider different types of questions:\n",
            "1. Factual questions about identified entities\n",
            "2. Conceptual questions about key terms\n",
            "3. Relationship questions between different concepts\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "import yake\n",
        "from typing import Dict, List, Tuple\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "\n",
        "class TextEnhancer:\n",
        "    def __init__(self):\n",
        "        # Load SpaCy model for NER and dependency parsing\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        # Initialize YAKE for keyword extraction\n",
        "        self.kw_extractor = yake.KeywordExtractor(\n",
        "            lan=\"en\",\n",
        "            n=3,  # ngram size\n",
        "            dedupLim=0.3,  # deduplication threshold\n",
        "            top=20,  # number of keywords to extract\n",
        "        )\n",
        "\n",
        "    def enhance_text(self, text: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Enhance input text with NER, key phrases, and importance scoring.\n",
        "        Returns structured data ready for LLM consumption.\n",
        "        \"\"\"\n",
        "        # Process text with SpaCy\n",
        "        doc = self.nlp(text)\n",
        "\n",
        "        # Extract named entities\n",
        "        entities = self._extract_entities(doc)\n",
        "\n",
        "        # Extract key phrases using YAKE\n",
        "        keywords = self._extract_keywords(text)\n",
        "\n",
        "        # Calculate term importance\n",
        "        term_importance = self._calculate_term_importance(text)\n",
        "\n",
        "        # Create enhanced context\n",
        "        enhanced_data = {\n",
        "            'original_text': text,\n",
        "            'entities': entities,\n",
        "            'key_phrases': keywords,\n",
        "            'term_importance': term_importance,\n",
        "            'enhanced_prompt': self._generate_enhanced_prompt(text, entities, keywords)\n",
        "        }\n",
        "\n",
        "        return enhanced_data\n",
        "\n",
        "    def _extract_entities(self, doc) -> Dict[str, List[str]]:\n",
        "        \"\"\"Extract and categorize named entities.\"\"\"\n",
        "        entities = {\n",
        "            'PERSON': [],\n",
        "            'ORG': [],\n",
        "            'DATE': [],\n",
        "            'GPE': [],  # Geographical/Political Entities\n",
        "            'CONCEPT': []  # Technical terms and concepts\n",
        "        }\n",
        "\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ in entities:\n",
        "                entities[ent.label_].append(ent.text)\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def _extract_keywords(self, text: str) -> List[Tuple[str, float]]:\n",
        "        \"\"\"Extract key phrases using YAKE algorithm.\"\"\"\n",
        "        keywords = self.kw_extractor.extract_keywords(text)\n",
        "        # Sort by score (lower is better in YAKE)\n",
        "        return sorted(keywords, key=lambda x: x[1])\n",
        "\n",
        "    def _calculate_term_importance(self, text: str) -> Dict[str, float]:\n",
        "        \"\"\"Calculate term importance using TF-IDF inspired approach.\"\"\"\n",
        "        # Create word frequency matrix\n",
        "        vectorizer = CountVectorizer(stop_words='english')\n",
        "        X = vectorizer.fit_transform([text])\n",
        "\n",
        "        # Get word frequencies\n",
        "        word_freq = dict(zip(vectorizer.get_feature_names_out(), X.toarray()[0]))\n",
        "\n",
        "        # Calculate importance score (normalized frequency)\n",
        "        total_words = sum(word_freq.values())\n",
        "        importance_scores = {\n",
        "            word: freq/total_words\n",
        "            for word, freq in word_freq.items()\n",
        "        }\n",
        "\n",
        "        return importance_scores\n",
        "\n",
        "    def _generate_enhanced_prompt(self, text: str, entities: Dict, keywords: List) -> str:\n",
        "        \"\"\"Generate an enhanced prompt for the LLM incorporating extracted information.\"\"\"\n",
        "        # Extract top 5 keywords\n",
        "        top_keywords = [kw[0] for kw in keywords[:5]]\n",
        "\n",
        "        # Create context string from entities\n",
        "        entity_context = []\n",
        "        for entity_type, values in entities.items():\n",
        "            if values:\n",
        "                entity_context.append(f\"{entity_type}: {', '.join(values)}\")\n",
        "\n",
        "        # Construct enhanced prompt\n",
        "        prompt = f\"\"\"\n",
        "Context:\n",
        "Important concepts: {', '.join(top_keywords)}\n",
        "{chr(10).join(entity_context)}\n",
        "\n",
        "Original text:\n",
        "{text}\n",
        "\n",
        "Please generate questions focusing on the relationships between these key concepts and entities.\n",
        "Consider different types of questions:\n",
        "1. Factual questions about identified entities\n",
        "2. Conceptual questions about key terms\n",
        "3. Relationship questions between different concepts\n",
        "\"\"\"\n",
        "        return prompt.strip()\n",
        "\n",
        "def process_pdf_chunk(chunk: str) -> Dict:\n",
        "    \"\"\"\n",
        "    Process a chunk of text from a PDF and prepare it for question generation.\n",
        "    \"\"\"\n",
        "    enhancer = TextEnhancer()\n",
        "    enhanced_data = enhancer.enhance_text(chunk)\n",
        "    return enhanced_data\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    sample_text = \"\"\"\n",
        "    The Industrial Revolution began in Britain in the late 18th century.\n",
        "    This period marked a major turning point in human history, fundamentally\n",
        "    changing economic and social organization. The transition included going\n",
        "    from manual production methods to machine manufacturing, new chemical\n",
        "    manufacturing and iron production processes, improved efficiency of water\n",
        "    power, the increasing use of steam power, and the development of machine tools.\n",
        "    \"\"\"\n",
        "\n",
        "    # Process the text\n",
        "    enhancer = TextEnhancer()\n",
        "    result = enhancer.enhance_text(sample_text)\n",
        "\n",
        "    # Example of how to use the enhanced data with an LLM\n",
        "    print(\"Enhanced prompt for LLM:\")\n",
        "    print(result['enhanced_prompt'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding domain-specific entity recognition for your subject area\n",
        "\n",
        "Implementing concept mapping between related terms\n",
        "\n",
        "Adding readability scoring to adjust question difficulty\n",
        "\n",
        "Implementing custom keyword extraction for educational content"
      ],
      "metadata": {
        "id": "VC5pI62ddgXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install spacy\n",
        "!pip install yake\n",
        "!pip install networkx\n",
        "!pip install nltk\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install scikit-learn\n",
        "\n",
        "# Import and download required NLTK data\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0zxq6rSeBry",
        "outputId": "67ddee78-ef9e-4f48-d6f7-81bbfcd52bb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.13.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: yake in /usr/local/lib/python3.10/dist-packages (0.4.8)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from yake) (0.9.0)\n",
            "Requirement already satisfied: click>=6.0 in /usr/local/lib/python3.10/dist-packages (from yake) (8.1.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from yake) (1.26.4)\n",
            "Requirement already satisfied: segtok in /usr/local/lib/python3.10/dist-packages (from yake) (1.5.11)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from yake) (3.4.2)\n",
            "Requirement already satisfied: jellyfish in /usr/local/lib/python3.10/dist-packages (from yake) (1.1.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from segtok->yake) (2024.9.11)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.4.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.13.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkyIoNyQea7P",
        "outputId": "878f7138-b6a9-4335-e7aa-61d177ffea52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from typing import List, Dict, Tuple\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import networkx as nx\n",
        "from collections import Counter\n",
        "\n",
        "class TextOptimizer:\n",
        "    def __init__(self, max_chunk_size: int = 1000):\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        self.max_chunk_size = max_chunk_size\n",
        "        self.tfidf = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "    def optimize_for_llm(self, text: str) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Main pipeline to optimize text for LLM processing.\n",
        "        Returns list of optimized chunks with metadata.\n",
        "        \"\"\"\n",
        "        # Break into initial chunks\n",
        "        sentences = sent_tokenize(text)\n",
        "        chunks = self._create_semantic_chunks(sentences)\n",
        "\n",
        "        # Process each chunk\n",
        "        optimized_chunks = []\n",
        "        for chunk in chunks:\n",
        "            # Get the most information-dense sentences\n",
        "            dense_text = self._extract_key_sentences(chunk)\n",
        "\n",
        "            # Remove redundant information\n",
        "            cleaned_text = self._remove_redundancy(dense_text)\n",
        "\n",
        "            # Structure the chunk with metadata\n",
        "            optimized_chunk = {\n",
        "                'original_length': len(chunk),\n",
        "                'optimized_length': len(cleaned_text),\n",
        "                'compression_ratio': len(cleaned_text) / len(chunk),\n",
        "                'content': cleaned_text,\n",
        "                'key_concepts': self._extract_key_concepts(cleaned_text)\n",
        "            }\n",
        "            optimized_chunks.append(optimized_chunk)\n",
        "\n",
        "        return optimized_chunks\n",
        "\n",
        "    def _create_semantic_chunks(self, sentences: List[str]) -> List[str]:\n",
        "        \"\"\"\n",
        "        Group sentences into semantic chunks based on topic similarity.\n",
        "        \"\"\"\n",
        "        # Calculate sentence embeddings using SpaCy\n",
        "        docs = [self.nlp(sent) for sent in sentences]\n",
        "        embeddings = np.array([doc.vector for doc in docs])\n",
        "\n",
        "        # Create similarity matrix\n",
        "        similarity_matrix = np.inner(embeddings, embeddings)\n",
        "\n",
        "        # Create graph from similarity matrix\n",
        "        G = nx.from_numpy_array(similarity_matrix)\n",
        "\n",
        "        # Find communities (chunks) using Louvain method\n",
        "        communities = nx.community.louvain_communities(G)\n",
        "\n",
        "        # Group sentences into chunks\n",
        "        chunks = []\n",
        "        for community in communities:\n",
        "            chunk_sentences = [sentences[i] for i in community]\n",
        "            chunk = ' '.join(chunk_sentences)\n",
        "\n",
        "            # Split if chunk is too large\n",
        "            if len(chunk) > self.max_chunk_size:\n",
        "                sub_chunks = self._split_chunk(chunk)\n",
        "                chunks.extend(sub_chunks)\n",
        "            else:\n",
        "                chunks.append(chunk)\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def _split_chunk(self, chunk: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Split large chunks while maintaining context.\n",
        "        \"\"\"\n",
        "        sentences = sent_tokenize(chunk)\n",
        "        current_chunk = []\n",
        "        chunks = []\n",
        "        current_length = 0\n",
        "\n",
        "        for sent in sentences:\n",
        "            if current_length + len(sent) > self.max_chunk_size:\n",
        "                chunks.append(' '.join(current_chunk))\n",
        "                current_chunk = [sent]\n",
        "                current_length = len(sent)\n",
        "            else:\n",
        "                current_chunk.append(sent)\n",
        "                current_length += len(sent)\n",
        "\n",
        "        if current_chunk:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def _extract_key_sentences(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Extract most informative sentences using TF-IDF scores.\n",
        "        \"\"\"\n",
        "        sentences = sent_tokenize(text)\n",
        "        if len(sentences) <= 3:\n",
        "            return text\n",
        "\n",
        "        # Calculate TF-IDF scores\n",
        "        tfidf_matrix = self.tfidf.fit_transform(sentences)\n",
        "        sentence_scores = np.sum(tfidf_matrix.toarray(), axis=1)\n",
        "\n",
        "        # Select top sentences\n",
        "        top_indices = np.argsort(sentence_scores)[-3:]\n",
        "        selected_sentences = [sentences[i] for i in sorted(top_indices)]\n",
        "\n",
        "        return ' '.join(selected_sentences)\n",
        "\n",
        "    def _remove_redundancy(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Remove redundant information while preserving context.\n",
        "        \"\"\"\n",
        "        doc = self.nlp(text)\n",
        "\n",
        "        # Track mentioned entities and concepts\n",
        "        mentioned = set()\n",
        "        filtered_sents = []\n",
        "\n",
        "        for sent in doc.sents:\n",
        "            # Extract key information from sentence\n",
        "            entities = {ent.text.lower() for ent in sent.ents}\n",
        "            noun_phrases = {np.text.lower() for np in sent.noun_chunks}\n",
        "            key_info = entities.union(noun_phrases)\n",
        "\n",
        "            # Check for redundancy\n",
        "            if len(key_info.intersection(mentioned)) < len(key_info) * 0.7:\n",
        "                filtered_sents.append(sent.text)\n",
        "                mentioned.update(key_info)\n",
        "\n",
        "        return ' '.join(filtered_sents)\n",
        "\n",
        "    def _extract_key_concepts(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Extract key concepts for context preservation.\n",
        "        \"\"\"\n",
        "        doc = self.nlp(text)\n",
        "\n",
        "        # Count noun phrases and entities\n",
        "        concepts = Counter()\n",
        "        for np in doc.noun_chunks:\n",
        "            concepts[np.text.lower()] += 1\n",
        "        for ent in doc.ents:\n",
        "            concepts[ent.text.lower()] += 2  # Weight entities higher\n",
        "\n",
        "        return [concept for concept, _ in concepts.most_common(5)]\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    optimizer = TextOptimizer(max_chunk_size=600)\n",
        "\n",
        "    sample_text = \"\"\"  [Photosynthesis, a fundamental process in the biosphere, is the conversion of light energy into chemical energy, primarily in plants, algae, and certain bacteria. This intricate process involves the absorption of sunlight, primarily by pigments like chlorophyll, and its subsequent utilization to synthesize organic compounds, primarily glucose, from inorganic substances like carbon dioxide and water. The by-product of this reaction is oxygen, a vital gas for most aerobic organisms.\n",
        "The photosynthetic process is broadly divided into two main stages: the light-dependent reactions and the light-independent reactions, or the Calvin cycle. In the light-dependent reactions, sunlight 1 is harnessed to excite electrons in chlorophyll molecules, initiating a chain of electron transport. This energy is used to split water molecules into hydrogen ions and oxygen, releasing the latter into the atmosphere. Additionally, ATP (adenosine triphosphate), an energy-rich molecule, and NADPH (nicotinamide adenine dinucleotide phosphate), a reducing agent, are synthesized.\n",
        "The light-independent reactions, or the Calvin cycle, occur in the stroma of the chloroplast, independent of light. Here, the ATP and NADPH generated in the light-dependent reactions provide the energy and reducing power, respectively, to fix carbon dioxide from the atmosphere. This process, known as carbon fixation, involves a series of enzyme-catalyzed reactions that ultimately lead to the formation of glucose. The fixed carbon is then used to synthesize other organic compounds like amino acids, fatty acids, and nucleotides, which are essential for plant growth and development.\n",
        "\n",
        "Photosynthesis is a crucial process for life on Earth. It provides the primary source of energy for most ecosystems, directly or indirectly. The oxygen produced during photosynthesis is essential for aerobic respiration, the process by which organisms obtain energy from organic molecules. Additionally, photosynthesis plays a significant role in the global carbon cycle, regulating the levels of carbon dioxide in the atmosphere and mitigating climate change.\n",
        "In conclusion, photosynthesis is a complex and vital process that sustains life on Earth. By harnessing the energy of sunlight, plants and other photosynthetic organisms convert inorganic substances into organic matter, providing the foundation for the food chain and regulating the Earth's atmosphere. Understanding the intricacies of photosynthesis is essential for addressing global challenges like climate change and food security.\n",
        "]  \"\"\"\n",
        "\n",
        "    optimized_chunks = optimizer.optimize_for_llm(sample_text)\n",
        "\n",
        "    for i, chunk in enumerate(optimized_chunks, 1):\n",
        "        print(f\"\\nChunk {i}:\")\n",
        "        print(f\"Compression ratio: {chunk['compression_ratio']:.2f}\")\n",
        "        print(f\"Key concepts: {', '.join(chunk['key_concepts'])}\")\n",
        "        print(\"Optimized content:\", chunk['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SW5VSr23cW5x",
        "outputId": "e0fae600-68a8-4808-f247-1a3cb31db618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chunk 1:\n",
            "Compression ratio: 1.00\n",
            "Key concepts: this intricate process, the absorption, sunlight, pigments, chlorophyll\n",
            "Optimized content: This intricate process involves the absorption of sunlight, primarily by pigments like chlorophyll, and its subsequent utilization to synthesize organic compounds, primarily glucose, from inorganic substances like carbon dioxide and water. It provides the primary source of energy for most ecosystems, directly or indirectly.\n",
            "\n",
            "Chunk 2:\n",
            "Compression ratio: 0.85\n",
            "Key concepts: photosynthesis, atp, nadph, two, calvin\n",
            "Optimized content:   [Photosynthesis, a fundamental process in the biosphere, is the conversion of light energy into chemical energy, primarily in plants, algae, and certain bacteria. The photosynthetic process is broadly divided into two main stages: the light-dependent reactions and the light-independent reactions, or the Calvin cycle. Additionally, ATP (adenosine triphosphate), an energy-rich molecule, and NADPH (nicotinamide adenine dinucleotide phosphate), a reducing agent, are synthesized.\n",
            "\n",
            "Chunk 3:\n",
            "Compression ratio: 1.00\n",
            "Key concepts: calvin, the light-independent reactions, the calvin cycle, the stroma, the chloroplast\n",
            "Optimized content: The light-independent reactions, or the Calvin cycle, occur in the stroma of the chloroplast, independent of light.\n",
            "\n",
            "Chunk 4:\n",
            "Compression ratio: 1.00\n",
            "Key concepts: earth, the energy, sunlight, plants, other photosynthetic organisms\n",
            "Optimized content: By harnessing the energy of sunlight, plants and other photosynthetic organisms convert inorganic substances into organic matter, providing the foundation for the food chain and regulating the Earth's atmosphere. Understanding the intricacies of photosynthesis is essential for addressing global challenges like climate change and food security.\n",
            " This energy is used to split water molecules into hydrogen ions and oxygen, releasing the latter into the atmosphere.\n",
            "\n",
            "Chunk 5:\n",
            "Compression ratio: 1.00\n",
            "Key concepts: 1, this process, carbon fixation, a series, enzyme-catalyzed reactions\n",
            "Optimized content: This process, known as carbon fixation, involves a series of enzyme-catalyzed reactions that ultimately lead to the formation of glucose. In the light-dependent reactions, sunlight 1 is harnessed to excite electrons in chlorophyll molecules, initiating a chain of electron transport.\n",
            "\n",
            "Chunk 6:\n",
            "Compression ratio: 1.00\n",
            "Key concepts: earth, photosynthesis, life, a crucial process, conclusion\n",
            "Optimized content: Photosynthesis is a crucial process for life on Earth. In conclusion, photosynthesis is a complex and vital process that sustains life on Earth.\n",
            "\n",
            "Chunk 7:\n",
            "Compression ratio: 1.00\n",
            "Key concepts: which, the fixed carbon, other organic compounds, amino acids, fatty acids\n",
            "Optimized content: The fixed carbon is then used to synthesize other organic compounds like amino acids, fatty acids, and nucleotides, which are essential for plant growth and development. The oxygen produced during photosynthesis is essential for aerobic respiration, the process by which organisms obtain energy from organic molecules.\n",
            "\n",
            "Chunk 8:\n",
            "Compression ratio: 1.00\n",
            "Key concepts: nadph, carbon dioxide, the atmosphere, atp, the atp\n",
            "Optimized content: Here, the ATP and NADPH generated in the light-dependent reactions provide the energy and reducing power, respectively, to fix carbon dioxide from the atmosphere. Additionally, photosynthesis plays a significant role in the global carbon cycle, regulating the levels of carbon dioxide in the atmosphere and mitigating climate change.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from typing import List, Dict\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import networkx as nx\n",
        "from collections import Counter\n",
        "\n",
        "class TextOptimizer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        max_chunk_size: int = 1000,\n",
        "        min_chunk_size: int = 200,\n",
        "        compression_target: float = 0.5,  # Target compression ratio (0.5 = 50% reduction)\n",
        "        similarity_threshold: float = 0.3,  # Threshold for semantic similarity\n",
        "        key_sentence_ratio: float = 0.3,   # Proportion of sentences to keep\n",
        "        redundancy_threshold: float = 0.5   # Threshold for redundant information\n",
        "    ):\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        self.max_chunk_size = max_chunk_size\n",
        "        self.min_chunk_size = min_chunk_size\n",
        "        self.compression_target = compression_target\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "        self.key_sentence_ratio = key_sentence_ratio\n",
        "        self.redundancy_threshold = redundancy_threshold\n",
        "        self.tfidf = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "    def optimize_for_llm(self, text: str) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Optimized pipeline with better compression control.\n",
        "        \"\"\"\n",
        "        # Initial sentence splitting\n",
        "        sentences = sent_tokenize(text)\n",
        "\n",
        "        # Create semantic chunks\n",
        "        chunks = self._create_semantic_chunks(sentences)\n",
        "\n",
        "        # Process each chunk\n",
        "        optimized_chunks = []\n",
        "        for chunk in chunks:\n",
        "            # Skip chunks that are too small\n",
        "            if len(chunk) < self.min_chunk_size:\n",
        "                continue\n",
        "\n",
        "            # Get the most important sentences\n",
        "            dense_text = self._extract_key_sentences(chunk)\n",
        "\n",
        "            # Remove redundant information\n",
        "            cleaned_text = self._remove_redundancy(dense_text)\n",
        "\n",
        "            # Only include chunk if it meets compression target\n",
        "            if len(cleaned_text) <= len(chunk) * self.compression_target:\n",
        "                optimized_chunks.append({\n",
        "                    'original_length': len(chunk),\n",
        "                    'optimized_length': len(cleaned_text),\n",
        "                    'compression_ratio': len(cleaned_text) / len(chunk),\n",
        "                    'content': cleaned_text,\n",
        "                    'key_concepts': self._extract_key_concepts(cleaned_text)\n",
        "                })\n",
        "\n",
        "        return optimized_chunks\n",
        "\n",
        "    def _create_semantic_chunks(self, sentences: List[str]) -> List[str]:\n",
        "        \"\"\"\n",
        "        Create chunks based on semantic similarity with improved control.\n",
        "        \"\"\"\n",
        "        if len(sentences) <= 3:\n",
        "            return [' '.join(sentences)]\n",
        "\n",
        "        # Calculate sentence embeddings\n",
        "        docs = [self.nlp(sent) for sent in sentences]\n",
        "        embeddings = np.array([doc.vector for doc in docs])\n",
        "\n",
        "        # Create similarity matrix\n",
        "        similarity_matrix = np.inner(embeddings, embeddings)\n",
        "\n",
        "        # Apply similarity threshold\n",
        "        similarity_matrix[similarity_matrix < self.similarity_threshold] = 0\n",
        "\n",
        "        # Create graph and find communities\n",
        "        G = nx.from_numpy_array(similarity_matrix)\n",
        "        communities = nx.community.louvain_communities(G)\n",
        "\n",
        "        # Process communities into chunks\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_length = 0\n",
        "\n",
        "        for community in communities:\n",
        "            community_sentences = [sentences[i] for i in sorted(community)]\n",
        "            community_text = ' '.join(community_sentences)\n",
        "\n",
        "            # Check if adding this community would exceed max chunk size\n",
        "            if current_length + len(community_text) > self.max_chunk_size:\n",
        "                if current_chunk:\n",
        "                    chunks.append(' '.join(current_chunk))\n",
        "                current_chunk = community_sentences\n",
        "                current_length = len(community_text)\n",
        "            else:\n",
        "                current_chunk.extend(community_sentences)\n",
        "                current_length += len(community_text)\n",
        "\n",
        "        if current_chunk:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def _extract_key_sentences(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Extract important sentences with configurable ratio.\n",
        "        \"\"\"\n",
        "        sentences = sent_tokenize(text)\n",
        "        if len(sentences) <= 3:\n",
        "            return text\n",
        "\n",
        "        # Calculate sentence importance\n",
        "        tfidf_matrix = self.tfidf.fit_transform(sentences)\n",
        "        sentence_scores = np.sum(tfidf_matrix.toarray(), axis=1)\n",
        "\n",
        "        # Select top sentences based on key_sentence_ratio\n",
        "        num_sentences = max(2, int(len(sentences) * self.key_sentence_ratio))\n",
        "        top_indices = np.argsort(sentence_scores)[-num_sentences:]\n",
        "        selected_sentences = [sentences[i] for i in sorted(top_indices)]\n",
        "\n",
        "        return ' '.join(selected_sentences)\n",
        "\n",
        "    def _remove_redundancy(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Remove redundant information with configurable threshold.\n",
        "        \"\"\"\n",
        "        doc = self.nlp(text)\n",
        "        mentioned = set()\n",
        "        filtered_sents = []\n",
        "\n",
        "        for sent in doc.sents:\n",
        "            entities = {ent.text.lower() for ent in sent.ents}\n",
        "            noun_phrases = {np.text.lower() for np in sent.noun_chunks}\n",
        "            key_info = entities.union(noun_phrases)\n",
        "\n",
        "            # Check redundancy against threshold\n",
        "            if not key_info or len(key_info.intersection(mentioned)) / len(key_info) < self.redundancy_threshold:\n",
        "                filtered_sents.append(sent.text)\n",
        "                mentioned.update(key_info)\n",
        "\n",
        "        return ' '.join(filtered_sents)\n",
        "\n",
        "# Example usage with different parameter combinations\n",
        "def test_optimization_parameters(text: str):\n",
        "    \"\"\"Test different parameter combinations and show results.\"\"\"\n",
        "    parameter_sets = [\n",
        "        # {\n",
        "        #     'compression_target': 0.3,  # Aggressive compression\n",
        "        #     'key_sentence_ratio': 0.2,\n",
        "        #     'redundancy_threshold': 0.3\n",
        "        # },\n",
        "        {\n",
        "            'compression_target': 0.5,  # Moderate compression\n",
        "            'key_sentence_ratio': 0.3,\n",
        "            'redundancy_threshold': 0.5\n",
        "        },\n",
        "        {\n",
        "            'compression_target': 0.7,  # Light compression\n",
        "            'key_sentence_ratio': 0.4,\n",
        "            'redundancy_threshold': 0.7\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    for params in parameter_sets:\n",
        "        print(f\"\\nTesting parameters: {params}\")\n",
        "        optimizer = TextOptimizer(**params)\n",
        "        chunks = optimizer.optimize_for_llm(text)\n",
        "\n",
        "        total_original = sum(chunk['original_length'] for chunk in chunks)\n",
        "        total_optimized = sum(chunk['optimized_length'] for chunk in chunks)\n",
        "\n",
        "        print(f\"Number of chunks: {len(chunks)}\")\n",
        "        print(f\"Overall compression ratio: {total_optimized/total_original:.2f}\")\n",
        "        print(f\"Original total length: {total_original}\")\n",
        "        print(f\"Optimized total length: {total_optimized}\")\n",
        "\n",
        "# Test with sample text\n",
        "if __name__ == \"__main__\":\n",
        "    sample_text = \"\"\"\n",
        "    [Photosynthesis, a fundamental process in the biosphere, is the conversion of light energy into chemical energy, primarily in plants, algae, and certain bacteria. This intricate process involves the absorption of sunlight, primarily by pigments like chlorophyll, and its subsequent utilization to synthesize organic compounds, primarily glucose, from inorganic substances like carbon dioxide and water. The by-product of this reaction is oxygen, a vital gas for most aerobic organisms.\n",
        "The photosynthetic process is broadly divided into two main stages: the light-dependent reactions and the light-independent reactions, or the Calvin cycle. In the light-dependent reactions, sunlight 1 is harnessed to excite electrons in chlorophyll molecules, initiating a chain of electron transport. This energy is used to split water molecules into hydrogen ions and oxygen, releasing the latter into the atmosphere. Additionally, ATP (adenosine triphosphate), an energy-rich molecule, and NADPH (nicotinamide adenine dinucleotide phosphate), a reducing agent, are synthesized.\n",
        "The light-independent reactions, or the Calvin cycle, occur in the stroma of the chloroplast, independent of light. Here, the ATP and NADPH generated in the light-dependent reactions provide the energy and reducing power, respectively, to fix carbon dioxide from the atmosphere. This process, known as carbon fixation, involves a series of enzyme-catalyzed reactions that ultimately lead to the formation of glucose. The fixed carbon is then used to synthesize other organic compounds like amino acids, fatty acids, and nucleotides, which are essential for plant growth and development.\n",
        "\n",
        "Photosynthesis is a crucial process for life on Earth. It provides the primary source of energy for most ecosystems, directly or indirectly. The oxygen produced during photosynthesis is essential for aerobic respiration, the process by which organisms obtain energy from organic molecules. Additionally, photosynthesis plays a significant role in the global carbon cycle, regulating the levels of carbon dioxide in the atmosphere and mitigating climate change.\n",
        "In conclusion, photosynthesis is a complex and vital process that sustains life on Earth. By harnessing the energy of sunlight, plants and other photosynthetic organisms convert inorganic substances into organic matter, providing the foundation for the food chain and regulating the Earth's atmosphere. Understanding the intricacies of photosynthesis is essential for addressing global challenges like climate change and food security.\n",
        "]\n",
        "    \"\"\"\n",
        "\n",
        "    test_optimization_parameters(sample_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "SV2jiDOZdvm2",
        "outputId": "1c794c20-cd7b-45f9-848e-f1e5c603d43b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing parameters: {'compression_target': 0.5, 'key_sentence_ratio': 0.3, 'redundancy_threshold': 0.5}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'TextOptimizer' object has no attribute '_extract_key_concepts'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-af6ddb1e03ba>\u001b[0m in \u001b[0;36m<cell line: 182>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m     \"\"\"\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0mtest_optimization_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-af6ddb1e03ba>\u001b[0m in \u001b[0;36mtest_optimization_parameters\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nTesting parameters: {params}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_for_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mtotal_original\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'original_length'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-af6ddb1e03ba>\u001b[0m in \u001b[0;36moptimize_for_llm\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     56\u001b[0m                     \u001b[0;34m'compression_ratio'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_text\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                     \u001b[0;34m'content'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcleaned_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                     \u001b[0;34m'key_concepts'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_key_concepts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                 })\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'TextOptimizer' object has no attribute '_extract_key_concepts'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextOptimizer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        max_chunk_size: int = 1000,\n",
        "        min_chunk_size: int = 50,\n",
        "        compression_target: float = 0.5,\n",
        "        similarity_threshold: float = 0.3,\n",
        "        key_sentence_ratio: float = 0.3,\n",
        "        redundancy_threshold: float = 0.5\n",
        "    ):\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        self.max_chunk_size = max_chunk_size\n",
        "        self.min_chunk_size = min_chunk_size\n",
        "        self.compression_target = compression_target\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "        self.key_sentence_ratio = key_sentence_ratio\n",
        "        self.redundancy_threshold = redundancy_threshold\n",
        "        self.tfidf = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "    def optimize_for_llm(self, text: str) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Optimized pipeline with better compression control.\n",
        "        \"\"\"\n",
        "        # Initial sentence splitting\n",
        "        sentences = sent_tokenize(text)\n",
        "\n",
        "        # Create semantic chunks\n",
        "        chunks = self._create_semantic_chunks(sentences)\n",
        "\n",
        "        # Process each chunk\n",
        "        optimized_chunks = []\n",
        "        for chunk in chunks:\n",
        "            # Get the most important sentences\n",
        "            dense_text = self._extract_key_sentences(chunk)\n",
        "\n",
        "            # Remove redundant information\n",
        "            cleaned_text = self._remove_redundancy(dense_text)\n",
        "\n",
        "            # Only include chunk if it meets compression target and isn't too small\n",
        "            if len(cleaned_text) > self.min_chunk_size:\n",
        "                optimized_chunks.append({\n",
        "                    'original_length': len(chunk),\n",
        "                    'optimized_length': len(cleaned_text),\n",
        "                    'compression_ratio': len(cleaned_text) / len(chunk),\n",
        "                    'content': cleaned_text,\n",
        "                    'key_concepts': self._extract_key_concepts(cleaned_text)\n",
        "                })\n",
        "\n",
        "        return optimized_chunks\n",
        "\n",
        "    def _create_semantic_chunks(self, sentences: List[str]) -> List[str]:\n",
        "        \"\"\"\n",
        "        Create chunks based on semantic similarity with improved control.\n",
        "        \"\"\"\n",
        "        if len(sentences) <= 3:\n",
        "            return [' '.join(sentences)]\n",
        "\n",
        "        # Calculate sentence embeddings\n",
        "        docs = [self.nlp(sent) for sent in sentences]\n",
        "        embeddings = np.array([doc.vector for doc in docs])\n",
        "\n",
        "        # Create similarity matrix\n",
        "        similarity_matrix = np.inner(embeddings, embeddings)\n",
        "\n",
        "        # Apply similarity threshold\n",
        "        similarity_matrix[similarity_matrix < self.similarity_threshold] = 0\n",
        "\n",
        "        # Create graph and find communities\n",
        "        G = nx.from_numpy_array(similarity_matrix)\n",
        "\n",
        "        try:\n",
        "            communities = nx.community.louvain_communities(G)\n",
        "        except:\n",
        "            # If community detection fails, create simple chunks\n",
        "            return [' '.join(sentences[i:i+5]) for i in range(0, len(sentences), 5)]\n",
        "\n",
        "        # Process communities into chunks\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_length = 0\n",
        "\n",
        "        for community in communities:\n",
        "            community_sentences = [sentences[i] for i in sorted(community)]\n",
        "            community_text = ' '.join(community_sentences)\n",
        "\n",
        "            if current_length + len(community_text) > self.max_chunk_size and current_chunk:\n",
        "                chunks.append(' '.join(current_chunk))\n",
        "                current_chunk = community_sentences\n",
        "                current_length = len(community_text)\n",
        "            else:\n",
        "                current_chunk.extend(community_sentences)\n",
        "                current_length += len(community_text)\n",
        "\n",
        "        if current_chunk:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "\n",
        "        # If no chunks were created, return the original text as one chunk\n",
        "        return chunks if chunks else [' '.join(sentences)]\n",
        "\n",
        "    def _extract_key_sentences(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Extract important sentences with configurable ratio.\n",
        "        \"\"\"\n",
        "        sentences = sent_tokenize(text)\n",
        "        if len(sentences) <= 3:\n",
        "            return text\n",
        "\n",
        "        # Calculate sentence importance\n",
        "        tfidf_matrix = self.tfidf.fit_transform(sentences)\n",
        "        sentence_scores = np.sum(tfidf_matrix.toarray(), axis=1)\n",
        "\n",
        "        # Select top sentences based on key_sentence_ratio\n",
        "        num_sentences = max(2, int(len(sentences) * self.key_sentence_ratio))\n",
        "        top_indices = np.argsort(sentence_scores)[-num_sentences:]\n",
        "        selected_sentences = [sentences[i] for i in sorted(top_indices)]\n",
        "\n",
        "        return ' '.join(selected_sentences)\n",
        "\n",
        "    def _remove_redundancy(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Remove redundant information with configurable threshold.\n",
        "        \"\"\"\n",
        "        doc = self.nlp(text)\n",
        "        mentioned = set()\n",
        "        filtered_sents = []\n",
        "\n",
        "        for sent in doc.sents:\n",
        "            entities = {ent.text.lower() for ent in sent.ents}\n",
        "            noun_phrases = {np.text.lower() for np in sent.noun_chunks}\n",
        "            key_info = entities.union(noun_phrases)\n",
        "\n",
        "            # Check redundancy against threshold\n",
        "            if not key_info or len(key_info.intersection(mentioned)) / len(key_info) < self.redundancy_threshold:\n",
        "                filtered_sents.append(sent.text)\n",
        "                mentioned.update(key_info)\n",
        "\n",
        "        return ' '.join(filtered_sents)\n",
        "\n",
        "    def _extract_key_concepts(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Extract key concepts for context preservation.\n",
        "        \"\"\"\n",
        "        doc = self.nlp(text)\n",
        "\n",
        "        # Count noun phrases and entities\n",
        "        concepts = Counter()\n",
        "\n",
        "        # Add noun phrases\n",
        "        for np in doc.noun_chunks:\n",
        "            if len(np.text.split()) <= 3:  # Limit to phrases of 3 words or less\n",
        "                concepts[np.text.lower()] += 1\n",
        "\n",
        "        # Add named entities with higher weight\n",
        "        for ent in doc.ents:\n",
        "            if len(ent.text.split()) <= 3:\n",
        "                concepts[ent.text.lower()] += 2\n",
        "\n",
        "        # Get most common concepts\n",
        "        return [concept for concept, _ in concepts.most_common(5)]\n",
        "\n",
        "\n",
        "def test_optimization_parameters(text: str):\n",
        "    \"\"\"Test different parameter combinations and show results.\"\"\"\n",
        "    parameter_sets = [\n",
        "        {\n",
        "            'compression_target': 0.3,\n",
        "            'key_sentence_ratio': 0.3,\n",
        "            'redundancy_threshold': 0.4,\n",
        "            'max_chunk_size': 800,\n",
        "            'min_chunk_size': 50\n",
        "        },\n",
        "        {\n",
        "            'compression_target': 0.5,\n",
        "            'key_sentence_ratio': 0.4,\n",
        "            'redundancy_threshold': 0.5,\n",
        "            'max_chunk_size': 800,\n",
        "            'min_chunk_size': 50\n",
        "        },\n",
        "        {\n",
        "            'compression_target': 0.7,\n",
        "            'key_sentence_ratio': 0.5,\n",
        "            'redundancy_threshold': 0.6,\n",
        "            'max_chunk_size': 800,\n",
        "            'min_chunk_size': 50\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    for params in parameter_sets:\n",
        "        print(f\"\\nTesting parameters: {params}\")\n",
        "        optimizer = TextOptimizer(**params)\n",
        "        chunks = optimizer.optimize_for_llm(text)\n",
        "\n",
        "        if not chunks:\n",
        "            print(\"No chunks generated with these parameters - they may be too aggressive\")\n",
        "            continue\n",
        "\n",
        "        total_original = sum(chunk['original_length'] for chunk in chunks)\n",
        "        total_optimized = sum(chunk['optimized_length'] for chunk in chunks)\n",
        "\n",
        "        print(f\"Number of chunks: {len(chunks)}\")\n",
        "        print(f\"Overall compression ratio: {total_optimized/total_original:.2f}\")\n",
        "        print(f\"Original total length: {total_original}\")\n",
        "        print(f\"Optimized total length: {total_optimized}\")\n",
        "\n",
        "        # Print first chunk as example\n",
        "        if chunks:\n",
        "            print(\"\\nFirst chunk example:\")\n",
        "            print(\"Content:\", chunks[0]['content'])\n",
        "            print(\"Key concepts:\", chunks[0]['key_concepts'])\n",
        "\n",
        "# Test with your sample text\n",
        "test_optimization_parameters(sample_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9EJrB6ylPce",
        "outputId": "8db21295-5bc8-4286-9305-5e3fe041fdc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing parameters: {'compression_target': 0.3, 'key_sentence_ratio': 0.3, 'redundancy_threshold': 0.4, 'max_chunk_size': 800, 'min_chunk_size': 50}\n",
            "Number of chunks: 3\n",
            "Overall compression ratio: 0.35\n",
            "Original total length: 2042\n",
            "Optimized total length: 716\n",
            "\n",
            "First chunk example:\n",
            "Content: The process occurs in two main stages: the light-dependent reactions and the light-independent reactions (Calvin cycle). This process, called carbon fixation, uses the ATP and NADPH produced in the light-dependent reactions.\n",
            "Key concepts: ['the light-dependent reactions', 'two', 'atp', 'the process', 'two main stages']\n",
            "\n",
            "Testing parameters: {'compression_target': 0.5, 'key_sentence_ratio': 0.4, 'redundancy_threshold': 0.5, 'max_chunk_size': 800, 'min_chunk_size': 50}\n",
            "Number of chunks: 4\n",
            "Overall compression ratio: 0.48\n",
            "Original total length: 2041\n",
            "Optimized total length: 987\n",
            "\n",
            "First chunk example:\n",
            "Content: Using the energy from sunlight, these materials are converted into glucose and oxygen is released as a byproduct. These sugars can then be used by the plant for energy or stored for later use.\n",
            "Key concepts: ['the energy', 'sunlight', 'these materials', 'glucose', 'oxygen']\n",
            "\n",
            "Testing parameters: {'compression_target': 0.7, 'key_sentence_ratio': 0.5, 'redundancy_threshold': 0.6, 'max_chunk_size': 800, 'min_chunk_size': 50}\n",
            "Number of chunks: 3\n",
            "Overall compression ratio: 0.55\n",
            "Original total length: 2042\n",
            "Optimized total length: 1124\n",
            "\n",
            "First chunk example:\n",
            "Content: Using the energy from sunlight, these materials are converted into glucose and oxygen is released as a byproduct. The process occurs in two main stages: the light-dependent reactions and the light-independent reactions (Calvin cycle). In the light-dependent reactions, sunlight is absorbed by chlorophyll in the thylakoid membranes. This process, called carbon fixation, uses the ATP and NADPH produced in the light-dependent reactions.\n",
            "Key concepts: ['the light-dependent reactions', 'sunlight', 'two', 'atp', 'the energy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def moderate_compression_output(text: str):\n",
        "    \"\"\"Run moderate compression on the entire text and return the optimized result.\"\"\"\n",
        "    # Define moderate compression parameters\n",
        "    params = {\n",
        "        'compression_target': 0.5,\n",
        "        'key_sentence_ratio': 0.4,\n",
        "        'redundancy_threshold': 0.5,\n",
        "        'max_chunk_size': 800,\n",
        "        'min_chunk_size': 50\n",
        "    }\n",
        "\n",
        "    # Initialize the optimizer with moderate parameters\n",
        "    optimizer = TextOptimizer(**params)\n",
        "\n",
        "    # Optimize the text\n",
        "    chunks = optimizer.optimize_for_llm(text)\n",
        "\n",
        "    if not chunks:\n",
        "        print(\"No optimized text generated. The parameters might be too restrictive.\")\n",
        "        return \"\"\n",
        "\n",
        "    # Merge all optimized chunks into one text output\n",
        "    optimized_text = \" \".join(chunk['content'] for chunk in chunks)\n",
        "\n",
        "    # Print overall compression details\n",
        "    total_original = sum(chunk['original_length'] for chunk in chunks)\n",
        "    total_optimized = sum(chunk['optimized_length'] for chunk in chunks)\n",
        "\n",
        "    print(\"Moderate Compression Results:\")\n",
        "    print(f\"Original total length: {total_original}\")\n",
        "    print(f\"Optimized total length: {total_optimized}\")\n",
        "    print(f\"Overall compression ratio: {total_optimized/total_original:.2f}\")\n",
        "\n",
        "    return optimized_text\n",
        "\n",
        "\n",
        "# Example usage with sample text\n",
        "optimized_result = moderate_compression_output(sample_text)\n",
        "print(\"\\nOptimized Text Output:\")\n",
        "print(optimized_result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GoRczFnlsc4",
        "outputId": "fc0a6533-427e-42df-f0e1-1139e19ad77c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moderate Compression Results:\n",
            "Original total length: 2042\n",
            "Optimized total length: 947\n",
            "Overall compression ratio: 0.46\n",
            "\n",
            "Optimized Text Output:\n",
            "Understanding this process is crucial for addressing challenges like food security and climate change in the modern world. The process occurs in two main stages: the light-dependent reactions and the light-independent reactions (Calvin cycle). This process, called carbon fixation, uses the ATP and NADPH produced in the light-dependent reactions. This process takes place in the chloroplasts, specifically using chlorophyll, the green pigment involved in photosynthesis. The glucose produced provides food not only for plants but also for animals that eat plants, forming the basis of most food chains on Earth. Using the energy from sunlight, these materials are converted into glucose and oxygen is released as a byproduct. The hydrogen ions help create ATP, while electrons from the split water molecules are used to form NADPH. Scientists continue to study photosynthesis to better understand its mechanisms and potentially improve crop yields.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wJ-lgGZEnxCj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}